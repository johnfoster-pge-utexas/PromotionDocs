---
title: Research Statement
author: John T. Foster 
---

# Overview 

I am primarily known for my theoretical and computational contributions to the relatively new field of peridynamics research.  I am one of the [highest cited researchers](https://scholar.google.com/citations?view_op=search_authors&hl=en&mauthors=label:peridynamics) in the field and am the [fourth largest code contributor](https://github.com/peridigm/peridigm/graphs/contributors) to the most popular open source peridyamics code, [*Peridigm*](https://github.com/peridigm/peridigm) (>62k lines of code contributed, additionally UT graduate students have contribued >10k lines of code under my supervision). Peridynamics is one of the leading computational methods for modeling dynamic and pervasive material failure. It has been used at Boeing for composite material aircraft design, at Corning for the evaluation of glass in iPhones, and has been implemented in the popular commercial FEA software LS-DYNA as well as several other open-source codes. *Peridigm* has been [used and cited in hundreds of studies/articles](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C44&q=peridigm&oq=) by researchers all over the world.  I have worked on problems involving both brittle and ductile failure, participated in several [Sandia Fracture Challenges](https://link.springer.com/article/10.1007/s10704-019-00361-1), published several papers on [new constitutive](https://www.sciencedirect.com/science/article/pii/S0020768318300829?via%3Dihub) and [failure theories](https://www.dl.begellhouse.com/journals/61fd1b191cf7e96f,5dbda3444284ba35,7d22e9255b968290.html), [multiphysics](https://www.sciencedirect.com/science/article/pii/S0021999113008474?via%3Dihub) and [multiscale](https://dx.doi.org/10.1007/s42102-020-00037-8) modeling, [hydraulic fracturing](https://link.springer.com/article/10.1007/s00466-015-1123-8), and recently published [extensive numerical validation experiments](https://www.sciencedirect.com/science/article/abs/pii/S0022509622001442?via%3Dihub) on using peridynamics for complex concrete modeling effectively putting to rest some long standing (and [high profile](https://scholar.google.com/citations?user=tJ4t4Q8AAAAJ&hl=en&oi=ao)) [criticisms](https://asmedigitalcollection.asme.org/appliedmechanics/article-abstract/89/6/061008/1139865/Critical-Comparison-of-Phase-Field-Peridynamics) of peridynamics.

My research has been supported through competitive grants from ARO, AFOSR through prestigious Young Investigator and MURI grants, Sandia National Laboratories and numerous energy companies. Recently, I shifted my research to focus on scientific machine learning applications in petroleum engineering, co-founding the DiReCT IAP research consortium with Prof. Michael Pyrcz in 2019. Our IAP is one of the most successful new IAPs in the Hildebrand Department with 9 companies (Chevron, ExxonMobile, Coterra, Equinor, Shell, Total, Japex, BP, Aramco) as current members each contributing $60k/year.  Prof. Pyrcz leads the administrative aspects of the IAP, e.g. billing and organizing our annual meeting; whereas, I lead our organized efforts for open-source software distribution by administering our [Github organization](https://github.com/UT-DIRECT) and instructing IAP graduate students on proper software engineering operations.  We both supervise graduate student research projects individually and as co-supervisors (currently co-supervising 3 students together).  Broadly, the IAP is focusing on data science and machine learning novel methods development to applications in petroleum engineering.  Prof. Pyrcz's research focus is primarly on geostatistics and petroleum reservoir modeling; whereas, I am working on incorporating physical constraints (e.g. engineering conservation laws) into machine learning methods.  Our backgrounds and expertise compliment each other well, and we believe the IAP will provide sustained funding for many years into the future given the promise and interest of using data science and machine learning, i.e. "Artificial Intelligence", for petroleum engineering decision making.

# Publications 
I have published more than 54 peer reviewed journal articles[^1] (>32 in rank)  and book chapters. I co-edited and contributed to the book, <u>[Handbook of Peridynamic Modeling](https://www.routledge.com/Handbook-of-Peridynamic-Modeling/Bobaru-Foster-Geubelle-Silling/p/book/9781482230437)</u> (>390 citations). I have consistently published in the top journals in mechanics and computational mechanics including [CMAME](https://www.sciencedirect.com/journal/computer-methods-in-applied-mechanics-and-engineering) (IF 6.6), [JMPS](https://www.sciencedirect.com/journal/journal-of-the-mechanics-and-physics-of-solids) (IF: 5.5), [JCP](https://www.sciencedirect.com/journal/journal-of-computational-physics) (IF: 4.4), [*Computational Mechanics*](https://www.springer.com/journal/466) (IF: 4.4), [IJSS](https://www.sciencedirect.com/journal/international-journal-of-solids-and-structures) (IF: 3.7) and others*.* One significant paper was on the front page of the [respective journal](https://www.begellhouse.com/journals/multiscale-computational-engineering.html) as a "most downloaded article" continuously for over 10 years[^2]<sup>,</sup>[^3]! Another paper, on nonlocal frictional contact, is [currently on the most downloaded page](https://www.springer.com/journal/42102/updates/18955096) in the *Journal of Peridynamics and Nonlocal Modeling*. My overall citations are strong (~2900 citations) and growing, receiving more than 500 citations in each of the last two years (on pace for >600 citations in 2023). My $h$-index is currently 27, I have 8 published works with more than 100 citations and 3 with more that 200 citations. Finally, it's worth noting that my citations have continued to grow in an upward trajectory given the field of peridynamics has been [growing rapidly](https://link.springer.com/article/10.1007/s42102-022-00082-5).  A screen capture of my [Google Scholar](https://scholar.google.com/citations?user=TjdPwewAAAAJ&hl=en) citations chart is shown in [@fig-scholar].

![Google Scholar Screen Capture - 7/2/2023](./img/google_scholar.png){#fig-scholar}


# Significant Research in Rank

What I consider to be one of my most significant works in rank was recently published in the *Journal of Mechanics and Physics of Solids* (paper 26 on CV). This paper implements an advanced microplane constitutive model (M7) developed by Prof. Zdeněk Bažant (NAS, NAE, AAAS, Royal Society of London, 7 honorary doctorates, etc.) and students over many years for concrete and other quasi-brittle materials. Prof. Bažant has long been a critic of peridynamics. After many unsuccessful conversations trying to convince him that peridynamics could be viewed as a regularized discritization technique that allows for straightforward modeling of material failure without mass loss, I initiated a research collaboration with Prof. Yuri Bazilevs and Dr. Masoud Behzadinasab at Brown University to demonstrate this claim clearly. Masoud was a recent UT-Engineering Mechanics PhD who graduated under my supervision. His PhD work had all the foundational elements for implementing the M7 model easily in our open source *Peridigm* code. Masoud and I implemented the M7 model that is distributed by Prof. Bažant as an Abaqus UMAT material model written in Fortran 90. Without changing a single line of the constitutive model code, we built a UMAT interface to *Peridigm* and ran 11 different numerical validation experiments. These tests all involved different stress loading paths and various fracture/notch tests to elucidate the effects of pressure dependence, stress triaxiality, size effects, and other factors on concrete failure. The results we published demonstrate good agreement with all 11 tests and the published computational results from Prof. Bažant's group using a regularized finite element implementation. Unlike the finite element results, our peridynamics simulations also show fracture localization and fracture patterns that are more in agreement with experiments. I see this work as the culmination of many years of constitutive model development from my group to provide a stable and accurate discretization technique that can use any classical stress-strain constitutive model for fracture modeling. The papers directly related to kinematic measures and constitutive model development that lead to this work are papers 3, 8, 9, 10, 12, 17 on my CV. Additionally, papers 4, 16, 18 were related to high-order discretization techniques that were demonstrated to produce the numerically stable formulation used in this work. When this work was published, [Dr. Stewart Silling](https://scholar.google.com/citations?user=XAjk6swAAAAJ&hl=en&oi=ao) from Sandia National Laboratories, the originator of peridynamic theory and Distingushed Member of the Technical Staff (highest technical rank at Sandia) with over 18k citations, wrote us and personally thanked us for writing the paper, with the sentiment that the paper rebutted Prof. Bažant's criticisms.  A screenshot of this email is shown in [@fig-stewart-note].

![Email from Dr. Stewart Silling](./img/stewart_note.png){#fig-stewart-note}

One of the papers mentioned above, paper 18, I also consider to be of high-significance. It was published in the *SIAM Journal of Numerical Analysis*. In that paper we present a discretization technique for nonlocal models based on reproducing kernel approximation theory that is proven to be "asymptotically compatible," this means that the discrete solution converges to the nonlocal solution as the characteristic nonlocality is held fixed and the mesh is refined *and* converges to the local solution as the characteristic nonlocality vanishes along with mesh refinement. Additionally, it's a nodally integrated collocation scheme; therefore it's more computationally efficient than using Gauss integration techniques. We prove stability and convergence of the scheme by comparing to the Galerkin discretization of the nonlocal model which has previously proven to be stable. This paper demonstrates the mathematical rigor our group has applied to nonlocal computational mechanics novel method development.

Finally, with my shift in research focus towards scientific machine learning in 2019, I have published several papers on the topic including papers 20, 21, 27, 29--31. I believe the most significant of these is paper 25, published in *Computer Methods in Applied Mechanics and Engineering* in 2022. This paper presents a novel physics-informed neural network (PINN) architecture for uncertainty quantification. The idea is quite elegant in its simplicity and performs remarkably well. We construct a feed-forward neural network and add physical balance laws described by partial differential equations (PDEs) into the loss function that is minimized during training to "learn" the solution. The key novelty is that instead of a single output corresponding to the solution of the PDE, we allow for many outputs. Each output corresponds to the solution of the balance law for training data that has been sampled from a distribution. The data could correspond to uncertain measurements of the field quantities of the balance law, uncertain measurements of boundary/initial conditions, or uncertainty in material parameters depending upon whether a forward or inverse solution is desired (both are demonstrated in the paper). We show that we can train *a single network* to produce an ensemble of solutions that form a distribution. We demonstrate that this distribution agrees well with Monte Carlo simulations conducted by solving the PDEs via the finite element method. We present a method for quantitatively comparing the two distributions with accuracy measures. While PINN methods, when training time is amortized over some number of forward solution runs, often result in longer solution times with lower accuracy than traditional PDE solvers for forward problems (e.g. FEM), the MO-PINN technique can produce the uncertain distributions faster than traditional FEM-based Monte Carlo techniques. This is especially true for inverse problems. This technique has the potential for transformative impact in the field of uncertainty quantification for computational science and engineering.

# Research related honors 

I received the 2023 Society of Petroleum Engineers Regional Data Science and Engineering Award. In 2018, I was awarded a Moncrief Grand Challenge Award from the Oden Institute for Computational Engineering and Sciences. I received a 2013 AFOSR Young Investigator Award, was named to the 2013 '40 Under 40' from the San Antonio Business Journal. I have given numerous invited talks at both national and international symposium and elite universities (Caltech, Northwestern, Johns Hopkins, Illinois, Vanderbilt, etc.). I recently delivered a conference plenary lecture at [Africomp 5](https://web.archive.org/web/20220816224636/https://africomp.info/speaker/). Among the other plenary speakers were [Wing Kam Lui](https://scholar.google.com/citations?user=lzQvgDoAAAAJ&hl=en&oi=ao), a chaired professor at Northwestern University with >60k citations, and [Peter Wriggers](https://scholar.google.com/citations?user=ERQy-O0AAAAJ&hl=en&oi=ao), >30k citations and widely viewed as one of the top technical and organizational leaders in computational mechanics in Europe, à la Tinsely Oden or Tom Hughes in the US.

# Grants

```{python}
from IPython.display import display, Markdown
import pandas as pd
import numpy as np
df = pd.read_csv('./grants.csv', delimiter='|')
select = ((df['Status'] != 'In Review').to_numpy() *
          (df['Rank'] == 'Associate').to_numpy())
total = np.round(df.loc[select, 'Project Total'].sum() / 1.0e6, decimals=2)
my_total = np.round(df.loc[select, 'Candidates Share'].sum() / 1.0e6, decimals=2)
display(Markdown(f'The tables below show research grants that have been awarded, pending, or proposed in the rank of associate professor.  The total funds for the awarded and pending projects is ${total} MM and my share of those awards is ${my_total} MM.'))
```



```{python}
#| label: tbl-current
#| tbl-cap: External Grants and Contracts Awarded in Rank - Current
import pandas as pd

def period(df):
  return f'{df["Start"]}-{df["End"]}'

df = pd.read_csv('./grants.csv', delimiter='|') 
df['Start_Sort'] = pd.to_datetime(df['Start'], format='%m/%Y')
df = df.sort_values(by=['Rank', 'Start_Sort'], ignore_index=True)

select = ((df['Status'] == 'Current').to_numpy() * 
          (df['IAP'] == False).to_numpy() *
          (df['Rank'] == 'Associate').to_numpy())
grant_period = df.apply(period, axis=1)
df = df.loc[select, 'Role of Candidate':'Candidates Share']
df['Period'] = grant_period
df.loc['TOTAL']= df.sum(numeric_only=True, axis=0)
df[['Project Total','Candidates Share']]= df[['Project Total','Candidates Share']].astype(int)
df.loc['TOTAL','Role of Candidate'] = 'TOTAL'
df = df.fillna('')
df.style.format({
  'Project Total': "${:,d}",
  'Candidates Share': "${:,d}"
}).hide(axis='index')
```

```{python}
#| label: tbl-completed
#| tbl-cap: External Grants and Contracts Awarded in Rank - Completed
df = pd.read_csv('./grants.csv', delimiter='|') 
df['Start_Sort'] = pd.to_datetime(df['Start'], format='%m/%Y')
df = df.sort_values(by=['Rank', 'Start_Sort'], ignore_index=True)

select = ((df['Status'] == 'Completed').to_numpy() * 
          (df['IAP'] == False).to_numpy() *
          (df['Rank'] == 'Associate').to_numpy())
grant_period = df.apply(period, axis=1)
df = df.loc[select, 'Role of Candidate':'Candidates Share']
df['Period'] = grant_period
df.loc['TOTAL']= df.sum(numeric_only=True, axis=0)
df[['Project Total','Candidates Share']]= df[['Project Total','Candidates Share']].astype(int)
df.loc['TOTAL','Role of Candidate'] = 'TOTAL'
df = df.fillna('')
df.style.format({
  'Project Total': "${:,d}",
  'Candidates Share': "${:,d}"
}).hide(axis='index')
```

```{python}
#| label: tbl-pending
#| tbl-cap: External Grants and Contracts Awarded in Rank - Pending
df = pd.read_csv('./grants.csv', delimiter='|') 
df['Start_Sort'] = pd.to_datetime(df['Start'], format='%m/%Y')
df = df.sort_values(by=['Rank', 'Start_Sort'], ignore_index=True)

select = ((df['Status'] == 'Pending').to_numpy() * 
          (df['IAP'] == False).to_numpy() *
          (df['Rank'] == 'Associate').to_numpy())
grant_period = df.apply(period, axis=1)
df = df.loc[select, 'Role of Candidate':'Candidates Share']
df['Period'] = grant_period
df.loc['TOTAL']= df.sum(numeric_only=True, axis=0)
df[['Project Total','Candidates Share']]= df[['Project Total','Candidates Share']].astype(int)
df.loc['TOTAL','Role of Candidate'] = 'TOTAL'
df = df.fillna('')
df.style.format({
  'Project Total': "${:,d}",
  'Candidates Share': "${:,d}"
}).hide(axis='index')
```

```{python}
#| label: tbl-inreview
#| tbl-cap: External Grants and Contracts Submitted in Rank - Under Review
df = pd.read_csv('./grants.csv', delimiter='|') 
df['Start_Sort'] = pd.to_datetime(df['Start'], format='%m/%Y')
df = df.sort_values(by=['Rank', 'Start_Sort'], ignore_index=True)

select = ((df['Status'] == 'In Review').to_numpy() * 
          (df['IAP'] == False).to_numpy() *
          (df['Rank'] == 'Associate').to_numpy())
grant_period = df.apply(period, axis=1)
df = df.loc[select, 'Role of Candidate':'Candidates Share']
df['Period'] = grant_period
df.loc['TOTAL']= df.sum(numeric_only=True, axis=0)
df[['Project Total','Candidates Share']]= df[['Project Total','Candidates Share']].astype(int)
df.loc['TOTAL','Role of Candidate'] = 'TOTAL'
df = df.fillna('')
df.style.format({
  'Project Total': "${:,d}",
  'Candidates Share': "${:,d}"
}).hide(axis='index')
```

```{python}
#| label: tbl-iap
#| tbl-cap: Industrial Affiliates Program and Gift Funding Awarded in Rank 
df = pd.read_csv('./grants.csv', delimiter='|') 
df['Start_Sort'] = pd.to_datetime(df['Start'], format='%m/%Y')
df = df.sort_values(by=['Rank', 'Start_Sort'], ignore_index=True)

select = ((df['IAP'] == True).to_numpy() *
          (df['Rank'] == 'Associate').to_numpy())
grant_period = df.apply(period, axis=1)
df = df.loc[select, 'Role of Candidate':'Candidates Share']
df['Period'] = grant_period
df.loc['TOTAL']= df.sum(numeric_only=True, axis=0)
df[['Project Total','Candidates Share']]= df[['Project Total','Candidates Share']].astype(int)
df.loc['TOTAL','Role of Candidate'] = 'TOTAL'
df = df.fillna('')
df.loc[:].style.format({
  'Project Total': "${:,d}",
  'Candidates Share': "${:,d}"
}).hide(axis='index')
```

```{python}
#| label: tbl-internal
#| tbl-cap: Internal Funding Awarded in Rank 
df = pd.read_csv('./grants.csv', delimiter='|') 
df['Start_Sort'] = pd.to_datetime(df['Start'], format='%m/%Y')
df = df.sort_values(by=['Rank', 'Start_Sort'], ignore_index=True)

select = ((df['Status'] == 'Internal').to_numpy() * 
          (df['IAP'] == False).to_numpy() *
          (df['Rank'] == 'Associate').to_numpy())
grant_period = df.apply(period, axis=1)
df = df.loc[select, 'Role of Candidate':'Candidates Share']
df['Period'] = grant_period
df.loc['TOTAL']= df.sum(numeric_only=True, axis=0)
df[['Project Total','Candidates Share']]= df[['Project Total','Candidates Share']].astype(int)
df.loc['TOTAL','Role of Candidate'] = 'TOTAL'
df = df.fillna('')
df.style.format({
  'Project Total': "${:,d}",
  'Candidates Share': "${:,d}"
}).hide(axis='index')
```


It's worth noting that I had two large grants, ARO ($345k) and AFOSR MURI (~$960k), that were awarded late in my assistant professor rank (therefore not shown in these tables, refer to CV) that carried over 2 and 3 years respectively after my tenure package was submitted.  If we amortize the research spending of all awarded grants over time, we can see that I actually spent far more in rank than is indicated in the tables.  This, along with the fact that my research is purely theoretical and computational (effectively only needing GRA support funds -- no equipment) is how I was able to support the **8 PhD students who graduated under my supervision in rank ** and is a primary reason that I wrote fewer proposals during the first few years of my associate professor rank. [@fig-cumulative] illustrates the cumulative research spending for my share of research grants since coming to academia.

Additionally, I shifted my research direction significantly in 2019 towards scientific machine learning applications in petroleum engineering co-founding the DiReCT IAP with Prof. Michael Pyrcz.  Our efforts in launching this IAP were severely impacted by the COVID19 pandemic (we launched with 5 companies committed to join and then had 3 summarily drop out in the Spring/Summer of 2020 due to budget concerns and widespread layoffs that affected the oil and gas industry during COVID).  However, the future looks bright as we now have 9 companies, including 3 that signed agreements to join just in the first half of 2023.  I have included a projection for future research spending (my share), assuming the 9 companies we currently have continue in the IAP as well as the pending proposals that have been notified of award ([@tbl-pending]) in [@fig-cumulative].

```{python}
#| label: fig-cumulative 
#| fig-cap: Cumulative Spending

import numpy as np
import matplotlib.pyplot as plt

def make_spend_df(df):
  dates = pd.date_range(start=df['Start'], periods=df['Days'])
  spend = np.ones(len(dates)) * df['$/day'] / 1.0e6
  return pd.Series(spend, index=dates)

df = pd.read_csv('./grants.csv', delimiter='|')
df['Start'] = pd.to_datetime(df['Start'], format='%m/%Y')
df['End'] = pd.to_datetime(df['End'], format='%m/%Y')
df['Days'] = (df['End'] - df['Start']).apply(lambda df: np.fix(df.days)).astype(np.int32)
df['$/day'] = df['Candidates Share'] / df['Days']


total_days = (pd.to_datetime('8/2023', format='%m/%Y') - df['Start'].min()).days
df_new = pd.DataFrame(index=pd.date_range(start=df['Start'].min(), periods=total_days))

select = (~(df['Status'] == 'In Review').to_numpy() * 
          ~(df['Status'] == 'Pending').to_numpy())

for index, row in df[select].iterrows():
   df_new[f'Project {index}'] = make_spend_df(row)


projected_dates = pd.date_range(start=pd.to_datetime('8/2023', format='%m/%Y'), periods=365*3 + 32)
projected1 = np.ones(len(projected_dates)) * df.loc[21, '$/day']
projected2 = np.ones(len(projected_dates)) * df.loc[19, '$/day']

projected_df = pd.DataFrame(data=dict(doe=projected1, direct=projected2), index=projected_dates)

fig = plt.figure()
ax = plt.gca()
select = df_new.index < pd.to_datetime('8/2016', format='%m/%Y')
pre_tenure_df = df_new.loc[select].sum(axis=1).cumsum()
pre_tenure_total = np.round(df_new.loc[select].sum(axis=1).sum(), decimals=2)
pre_tenure_df.plot(ax=ax)
ax.fill_between(pre_tenure_df.index.to_numpy(), pre_tenure_df.to_numpy(), hatch='/')
ax.text(pd.to_datetime('1/2013', format='%m/%Y'), 0.2, f'Total = ${pre_tenure_total} MM', bbox=dict(boxstyle="square,pad=0.3", fc="lightblue", ec="lightblue"))

select = df_new.index >= pd.to_datetime('8/2016', format='%m/%Y')
post_tenure_df = df_new.loc[select].sum(axis=1).cumsum() 
post_tenure_total = np.round(df_new.loc[select].sum(axis=1).sum(), decimals=2)
post_tenure_df = post_tenure_df + pre_tenure_df.iloc[-1]
post_tenure_df.plot(ax=ax)
ax.fill_between(post_tenure_df.index.to_numpy(), post_tenure_df.to_numpy(), hatch='+')

projected_df = projected_df.sum(axis=1).cumsum() / 1.0e6 + post_tenure_df.iloc[-1]
projected_df.plot(ax=ax, linestyle='--', color='black')

ax.text(pd.to_datetime('12/2018', format='%m/%Y'), 1.2, f'Total = ${post_tenure_total} MM', bbox=dict(boxstyle="square,pad=0.3", fc="orange", ec="orange"))
tenure_date = pd.to_datetime('8/2016', format='%m/%Y')
text_date = pd.to_datetime('5/2017', format='%m/%Y')
plt.axvline(x=tenure_date, color='black')
ax.annotate('Tenure package submitted', xy=(tenure_date, 4.4), xytext=(text_date, 4.35), arrowprops=dict(arrowstyle='->'))
projected_date = pd.to_datetime('1/2025', format='%m/%Y')
projected_text_date = pd.to_datetime('9/2023', format='%m/%Y')
ax.annotate('Projection using', xy=(projected_date, 3.9), xytext=(projected_text_date, 3.0), arrowprops=dict(arrowstyle='->'))
ax.text(projected_text_date, 2.8, 'IAP and Pending')
ax.set_xlabel('Years')
ax.set_ylabel('Cumulative Spending - My Share (MM$)');
```


[^1]: <http://johnfoster.pge.utexas.edu/cv/>
[^2]: [Wayback Machine Capture 1](https://web.archive.org/web/20130427051412/https://www.begellhouse.com/journals/multiscale-computational-engineering.html)
[^3]: [Wayback Machine Capture 2](https://web.archive.org/web/20220930072957/https://www.begellhouse.com/journals/multiscale-computational-engineering.html)
